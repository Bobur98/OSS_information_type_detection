{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested 5-Fold Cross Validation For Logistic Regression On Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd as xl\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import pprint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "import time, datetime\n",
    "from functools import partial, update_wrapper\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as Imb_Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score, make_scorer, confusion_matrix\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use spaCy parser for word tokenization of a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create an instance of the English parser\n",
    "parser = English()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define stopwords as punctuation + common contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(punctuation) + [\"'s\",\"'m\",\"n't\",\"'re\",\"-\",\"'ll\",'...'] #+ stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to lemmatize and tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(item):\n",
    "    return WordNetLemmatizer().lemmatize(item)\n",
    "\n",
    "def tokenize(line):\n",
    "    line_tokens = []\n",
    "    tokens = parser(line)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            line_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            line_tokens.append('SCREEN_NAME')\n",
    "        elif str(token) not in stop_words:\n",
    "            line_tokens.append(get_lemma(token.lower_))\n",
    "    return line_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus: 8428\n"
     ]
    }
   ],
   "source": [
    "### Read from the pickled file\n",
    "all_data = pd.read_excel('../data/combined_dataset.xlsx')\n",
    "\n",
    "print(\"Size of corpus: \"+str(len(all_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.dropna(subset=['Text Content', 'Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_remove = [ \"Testing\",'Future Plan','Issue Content Management']\n",
    "all_data = all_data[~all_data['Code'].isin(labels_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 13\n",
      "[   'Action on Issue',\n",
      "    'Bug Reproduction',\n",
      "    'Contribution and Commitment',\n",
      "    'Expected Behaviour',\n",
      "    'Investigation and Exploration',\n",
      "    'Motivation',\n",
      "    'Observed Bug Behaviour',\n",
      "    'Potential New Issues and Requests',\n",
      "    'Social Conversation',\n",
      "    'Solution Discussion',\n",
      "    'Solution Usage',\n",
      "    'Task Progress',\n",
      "    'Workarounds']\n"
     ]
    }
   ],
   "source": [
    "X = all_data['Text Content'].values\n",
    "y = all_data['Code'].values\n",
    "\n",
    "print(\"Number of unique labels: \"+str(len(set(y))))\n",
    "\n",
    "labels = list(set(y))\n",
    "labels.sort()\n",
    "\n",
    "pp.pprint(labels)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross-Validation on Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be used within GridSearch\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# To be used in outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline: tfidf + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Imb_Pipeline([\n",
    "    ('vect', TfidfVectorizer(tokenizer=tokenize)),\n",
    "    ('smote', SMOTE()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "### Hyperparameters to search\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__C': (0.01, 0.1, 1, 10),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross Validation using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Pipeline scenario in 0:27:26.571548\n"
     ]
    }
   ],
   "source": [
    "### Define and create the scoring functions\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "def score_func(y_true, y_pred, score_index, i):\n",
    "    return(precision_recall_fscore_support(y_true,y_pred)[score_index][i])\n",
    "\n",
    "def avg_score(y_true, y_pred, score_index):\n",
    "    return precision_recall_fscore_support(y_true,y_pred,average='weighted')[score_index]\n",
    "\n",
    "def sum_support(y_true, y_pred):\n",
    "    return len(y_true)\n",
    "\n",
    "### Create partials for each of the metrics returned\n",
    "score_funcs = {v: partial(score_func, score_index=k) for k, v in {0:'precision',1:'recall',2:'fscore',3:'support'}.items()}\n",
    "prec_score = partial(score_func, score_index=0)\n",
    "update_wrapper(prec_score,score_func)\n",
    "rec_score = partial(score_func, score_index=1)\n",
    "update_wrapper(rec_score,score_func)\n",
    "f_score = partial(score_func, score_index=2)\n",
    "update_wrapper(f_score,score_func)\n",
    "support_score = partial(score_func, score_index=3)\n",
    "update_wrapper(support_score,score_func)\n",
    "\n",
    "### Create a callable scoring function for each of the metrics for each classification label\n",
    "scorer = {}\n",
    "for label_id in range(0,13):\n",
    "    scorer['label'+str(label_id)+'_precision'] = make_scorer(prec_score, i=label_id)\n",
    "    scorer['label'+str(label_id)+'_recall'] = make_scorer(rec_score, i=label_id)\n",
    "    scorer['label'+str(label_id)+'_fscore'] = make_scorer(f_score, i=label_id)\n",
    "    scorer['label'+str(label_id)+'_support'] = make_scorer(support_score, i=label_id)\n",
    "\n",
    "### Create a callable scoring function for avg/total of the metrics across classification labels\n",
    "scorer['avg_precision'] = make_scorer(avg_score,score_index=0)\n",
    "scorer['avg_recall'] = make_scorer(avg_score,score_index=1)\n",
    "scorer['avg_fscore'] = make_scorer(avg_score,score_index=2)\n",
    "scorer['total_support'] = make_scorer(sum_support)\n",
    "\n",
    "\n",
    "### Perform Nested cross-validation on Pipeline\n",
    "start = time.time()\n",
    "clf = GridSearchCV(pipeline, parameters, cv=inner_cv, scoring='f1_weighted')\n",
    "clf_results = cross_validate(clf, X=X, y=y, cv=outer_cv, scoring=scorer)\n",
    "print(\"Completed Pipeline scenario in \"+ str(datetime.timedelta(seconds=(time.time()-start))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------- FOLD 0: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.68    0.62      0.65     21.0\n",
      "Bug Reproduction                        0.58    0.53      0.55    119.0\n",
      "Contribution and Commitment             0.78    0.74      0.76    131.0\n",
      "Expected Behaviour                      0.19    0.16      0.18     31.0\n",
      "Investigation and Exploration           0.56    0.39      0.46    152.0\n",
      "Motivation                              0.33    0.28      0.30     71.0\n",
      "Observed Bug Behaviour                  0.27    0.28      0.27     61.0\n",
      "Potential New Issues and Requests       0.28    0.22      0.25     50.0\n",
      "Social Conversation                     0.69    0.70      0.69    282.0\n",
      "Solution Discussion                     0.60    0.67      0.64    590.0\n",
      "Solution Usage                          0.35    0.54      0.43     76.0\n",
      "Task Progress                           0.43    0.25      0.32     36.0\n",
      "Workarounds                             0.34    0.28      0.31     46.0\n",
      "Avg/Total                               0.56    0.56      0.56   1666.0\n",
      "\n",
      "------------------------- FOLD 1: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.67    0.57      0.62     21.0\n",
      "Bug Reproduction                        0.50    0.55      0.52    119.0\n",
      "Contribution and Commitment             0.76    0.73      0.75    132.0\n",
      "Expected Behaviour                      0.30    0.19      0.24     31.0\n",
      "Investigation and Exploration           0.41    0.33      0.36    153.0\n",
      "Motivation                              0.41    0.31      0.35     71.0\n",
      "Observed Bug Behaviour                  0.38    0.27      0.31     60.0\n",
      "Potential New Issues and Requests       0.27    0.18      0.21     51.0\n",
      "Social Conversation                     0.73    0.75      0.74    281.0\n",
      "Solution Discussion                     0.60    0.70      0.65    590.0\n",
      "Solution Usage                          0.44    0.54      0.48     76.0\n",
      "Task Progress                           0.18    0.14      0.16     36.0\n",
      "Workarounds                             0.41    0.20      0.27     45.0\n",
      "Avg/Total                               0.56    0.57      0.56   1666.0\n",
      "\n",
      "------------------------- FOLD 2: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.82    0.67      0.74     21.0\n",
      "Bug Reproduction                        0.52    0.52      0.52    120.0\n",
      "Contribution and Commitment             0.75    0.74      0.75    132.0\n",
      "Expected Behaviour                      0.28    0.16      0.20     31.0\n",
      "Investigation and Exploration           0.50    0.41      0.45    152.0\n",
      "Motivation                              0.35    0.36      0.35     70.0\n",
      "Observed Bug Behaviour                  0.41    0.20      0.27     60.0\n",
      "Potential New Issues and Requests       0.24    0.18      0.20     51.0\n",
      "Social Conversation                     0.70    0.73      0.71    282.0\n",
      "Solution Discussion                     0.62    0.68      0.65    590.0\n",
      "Solution Usage                          0.41    0.62      0.49     76.0\n",
      "Task Progress                           0.31    0.22      0.26     36.0\n",
      "Workarounds                             0.31    0.24      0.27     45.0\n",
      "Avg/Total                               0.57    0.58      0.57   1666.0\n",
      "\n",
      "------------------------- FOLD 3: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.71    0.81      0.76     21.0\n",
      "Bug Reproduction                        0.48    0.51      0.49    120.0\n",
      "Contribution and Commitment             0.80    0.68      0.73    132.0\n",
      "Expected Behaviour                      0.37    0.23      0.28     31.0\n",
      "Investigation and Exploration           0.37    0.36      0.36    152.0\n",
      "Motivation                              0.35    0.33      0.34     70.0\n",
      "Observed Bug Behaviour                  0.27    0.21      0.24     61.0\n",
      "Potential New Issues and Requests       0.17    0.20      0.18     51.0\n",
      "Social Conversation                     0.70    0.73      0.71    282.0\n",
      "Solution Discussion                     0.61    0.63      0.62    590.0\n",
      "Solution Usage                          0.46    0.65      0.54     75.0\n",
      "Task Progress                           0.50    0.29      0.36     35.0\n",
      "Workarounds                             0.31    0.22      0.26     46.0\n",
      "Avg/Total                               0.55    0.55      0.55   1666.0\n",
      "\n",
      "------------------------- FOLD 4: -------------------------\n",
      "\n",
      "Training Results:\n",
      "Empty DataFrame\n",
      "Columns: [Precision, Recall, F1-score, Support]\n",
      "Index: []\n",
      "\n",
      "Test Results:\n",
      "                                   Precision  Recall  F1-score  Support\n",
      "Action on Issue                         0.53    0.48      0.50     21.0\n",
      "Bug Reproduction                        0.55    0.56      0.56    119.0\n",
      "Contribution and Commitment             0.75    0.67      0.71    131.0\n",
      "Expected Behaviour                      0.32    0.19      0.24     31.0\n",
      "Investigation and Exploration           0.48    0.38      0.43    152.0\n",
      "Motivation                              0.29    0.23      0.26     70.0\n",
      "Observed Bug Behaviour                  0.35    0.23      0.28     61.0\n",
      "Potential New Issues and Requests       0.29    0.22      0.25     51.0\n",
      "Social Conversation                     0.68    0.70      0.69    282.0\n",
      "Solution Discussion                     0.60    0.68      0.64    590.0\n",
      "Solution Usage                          0.39    0.61      0.47     75.0\n",
      "Task Progress                           0.44    0.31      0.36     36.0\n",
      "Workarounds                             0.16    0.11      0.13     46.0\n",
      "Avg/Total                               0.55    0.56      0.55   1665.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_report = pd.DataFrame(columns=['Precision', 'Recall', 'F1-score', 'Support'])\n",
    "test_report = pd.DataFrame(columns=['Precision', 'Recall', 'F1-score', 'Support'])\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "writer = pd.ExcelWriter('../results/combined_dataset_smote_result.xlsx')\n",
    "\n",
    "\n",
    "datalength = 0\n",
    "\n",
    "for i in range(0, 5):\n",
    "    for label_id in range(0, 13):\n",
    "        train_label_precision_key = 'train_label' + str(label_id) + '_precision'\n",
    "        train_label_recall_key = 'train_label' + str(label_id) + '_recall'\n",
    "        train_label_fscore_key = 'train_label' + str(label_id) + '_fscore'\n",
    "        train_label_support_key = 'train_label' + str(label_id) + '_support'\n",
    "\n",
    "        if train_label_precision_key in clf_results and train_label_recall_key in clf_results and \\\n",
    "            train_label_fscore_key in clf_results and train_label_support_key in clf_results:\n",
    "\n",
    "            train_report.loc[labels[label_id], :] = [clf_results[train_label_precision_key][i],\n",
    "                                                      clf_results[train_label_recall_key][i],\n",
    "                                                      clf_results[train_label_fscore_key][i],\n",
    "                                                      clf_results[train_label_support_key][i]]\n",
    "\n",
    "        test_label_precision_key = 'test_label' + str(label_id) + '_precision'\n",
    "        test_label_recall_key = 'test_label' + str(label_id) + '_recall'\n",
    "        test_label_fscore_key = 'test_label' + str(label_id) + '_fscore'\n",
    "        test_label_support_key = 'test_label' + str(label_id) + '_support'\n",
    "\n",
    "        if test_label_precision_key in clf_results and test_label_recall_key in clf_results and \\\n",
    "            test_label_fscore_key in clf_results and test_label_support_key in clf_results:\n",
    "\n",
    "            test_report.loc[labels[label_id], :] = [clf_results[test_label_precision_key][i],\n",
    "                                                     clf_results[test_label_recall_key][i],\n",
    "                                                     clf_results[test_label_fscore_key][i],\n",
    "                                                     clf_results[test_label_support_key][i]]\n",
    "\n",
    "    train_avg_precision_key = 'train_avg_precision'\n",
    "    train_avg_recall_key = 'train_avg_recall'\n",
    "    train_avg_fscore_key = 'train_avg_fscore'\n",
    "    train_total_support_key = 'train_total_support'\n",
    "\n",
    "    if train_avg_precision_key in clf_results and train_avg_recall_key in clf_results and \\\n",
    "        train_avg_fscore_key in clf_results and train_total_support_key in clf_results:\n",
    "\n",
    "        train_report.loc['Avg/Total', :] = [clf_results[train_avg_precision_key][i],\n",
    "                                             clf_results[train_avg_recall_key][i],\n",
    "                                             clf_results[train_avg_fscore_key][i],\n",
    "                                             clf_results[train_total_support_key][i]]\n",
    "\n",
    "    test_avg_precision_key = 'test_avg_precision'\n",
    "    test_avg_recall_key = 'test_avg_recall'\n",
    "    test_avg_fscore_key = 'test_avg_fscore'\n",
    "    test_total_support_key = 'test_total_support'\n",
    "\n",
    "    if test_avg_precision_key in clf_results and test_avg_recall_key in clf_results and \\\n",
    "        test_avg_fscore_key in clf_results and test_total_support_key in clf_results:\n",
    "\n",
    "        test_report.loc['Avg/Total', :] = [clf_results[test_avg_precision_key][i],\n",
    "                                            clf_results[test_avg_recall_key][i],\n",
    "                                            clf_results[test_avg_fscore_key][i],\n",
    "                                            clf_results[test_total_support_key][i]]\n",
    "\n",
    "    fold_index = pd.DataFrame(data=[{'Fold': 'Fold ' + str(i)}])\n",
    "    fold_index.to_excel(writer, 'LTC', startrow=datalength, index=False)\n",
    "    datalength += (len(fold_index) + 2)\n",
    "    train_report.to_excel(writer, 'LTC', startrow=datalength)\n",
    "    datalength += (len(train_report) + 2)\n",
    "    test_report.to_excel(writer, 'LTC', startrow=datalength)\n",
    "    datalength += (len(test_report) + 2)\n",
    "\n",
    "    result_dict['LTC_train_' + str(i)] = train_report\n",
    "    result_dict['LTC_test_' + str(i)] = test_report\n",
    "\n",
    "    train_report = train_report.astype(float).round(2)\n",
    "    test_report = test_report.astype(float).round(2)\n",
    "\n",
    "    print(\"\\n------------------------- FOLD \" + str(i) + \": -------------------------\")\n",
    "    print(\"\\nTraining Results:\")\n",
    "    print(train_report)\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(test_report)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Test Report Across 5 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Test Report Across 5 Folds:\n",
      "                                   Avg Precision  Avg Recall  Avg F1-score  \\\n",
      "Action on Issue                             0.68        0.63          0.65   \n",
      "Bug Reproduction                            0.52        0.53          0.53   \n",
      "Contribution and Commitment                 0.77        0.71          0.74   \n",
      "Expected Behaviour                          0.29        0.19          0.23   \n",
      "Investigation and Exploration               0.46        0.37          0.41   \n",
      "Motivation                                  0.34        0.30          0.32   \n",
      "Observed Bug Behaviour                      0.34        0.24          0.27   \n",
      "Potential New Issues and Requests           0.25        0.20          0.22   \n",
      "Social Conversation                         0.70        0.72          0.71   \n",
      "Solution Discussion                         0.61        0.67          0.64   \n",
      "Solution Usage                              0.41        0.59          0.48   \n",
      "Task Progress                               0.37        0.24          0.29   \n",
      "Workarounds                                 0.31        0.21          0.25   \n",
      "Total/Avg                                   0.56        0.57          0.56   \n",
      "\n",
      "                                   Total Support  \n",
      "Action on Issue                            105.0  \n",
      "Bug Reproduction                           597.0  \n",
      "Contribution and Commitment                658.0  \n",
      "Expected Behaviour                         155.0  \n",
      "Investigation and Exploration              761.0  \n",
      "Motivation                                 352.0  \n",
      "Observed Bug Behaviour                     303.0  \n",
      "Potential New Issues and Requests          254.0  \n",
      "Social Conversation                       1409.0  \n",
      "Solution Discussion                       2950.0  \n",
      "Solution Usage                             378.0  \n",
      "Task Progress                              179.0  \n",
      "Workarounds                                228.0  \n",
      "Total/Avg                                 8329.0  \n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames to hold the averages\n",
    "avg_test_report = pd.DataFrame(columns=['Avg Precision', 'Avg Recall', 'Avg F1-score', 'Total Support'], index=labels)\n",
    "\n",
    "# Variables to calculate weighted averages\n",
    "total_test_support = 0\n",
    "weighted_test_precision = 0\n",
    "weighted_test_recall = 0\n",
    "weighted_test_f1 = 0\n",
    "\n",
    "# Calculate averages across 5 folds for each label\n",
    "for label_id in range(13):\n",
    "\n",
    "    test_precisions = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'Precision'] for i in range(5)]\n",
    "    test_recalls = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'Recall'] for i in range(5)]\n",
    "    test_f1_scores = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'F1-score'] for i in range(5)]\n",
    "    test_supports = [result_dict['LTC_test_' + str(i)].loc[labels[label_id], 'Support'] for i in range(5)]\n",
    "\n",
    "    # Calculate averages for each label\n",
    "    avg_test_precision = np.mean(test_precisions)\n",
    "    avg_test_recall = np.mean(test_recalls)\n",
    "    avg_test_f1 = np.mean(test_f1_scores)\n",
    "    total_test_label_support = np.sum(test_supports)\n",
    "\n",
    "    avg_test_report.loc[labels[label_id]] = [avg_test_precision, avg_test_recall, avg_test_f1, total_test_label_support]\n",
    "\n",
    "    # Accumulate for weighted averages\n",
    "\n",
    "    weighted_test_precision += avg_test_precision * total_test_label_support\n",
    "    weighted_test_recall += avg_test_recall * total_test_label_support\n",
    "    weighted_test_f1 += avg_test_f1 * total_test_label_support\n",
    "    total_test_support += total_test_label_support\n",
    "\n",
    "# Calculate and add total weighted averages\n",
    "if total_test_support > 0:\n",
    "    avg_test_report.loc['Total/Avg', :] = [weighted_test_precision / total_test_support,\n",
    "                                           weighted_test_recall / total_test_support,\n",
    "                                           weighted_test_f1 / total_test_support,\n",
    "                                           total_test_support]\n",
    "\n",
    "# Convert to float and round to 2 decimal places\n",
    "avg_test_report = avg_test_report.astype(float).round(2)\n",
    "\n",
    "print(\"\\nAverage Test Report Across 5 Folds:\")\n",
    "print(avg_test_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
